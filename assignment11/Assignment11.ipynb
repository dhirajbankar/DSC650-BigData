{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "induced-florence",
   "metadata": {},
   "source": [
    "#### Text generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-sixth",
   "metadata": {},
   "source": [
    "This notebook contains the code samples found in Chapter 8, Section 1 of Deep Learning with Python. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-program",
   "metadata": {},
   "source": [
    "#### Implementing character-level LSTM text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-going",
   "metadata": {},
   "source": [
    "Let’s put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files – Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche’s writing style and topics of choice, rather than a more generic model of the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-ground",
   "metadata": {},
   "source": [
    "Let’s start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-haven",
   "metadata": {},
   "source": [
    "##### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heated-robin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 0s 1us/step\n",
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-subsection",
   "metadata": {},
   "source": [
    "Next, we will extract partially-overlapping sequences of length maxlen, one-hot encode them and pack them in a 3D Numpy array x of shape (sequences, maxlen, unique_characters). Simultaneously, we prepare a array y containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "underlying-wayne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-benefit",
   "metadata": {},
   "source": [
    "##### Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-creation",
   "metadata": {},
   "source": [
    "Our network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters. But let us note that recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strategic-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-beatles",
   "metadata": {},
   "source": [
    "Since our targets are one-hot encoded, we will use categorical_crossentropy as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collectible-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-madagascar",
   "metadata": {},
   "source": [
    "##### Training the language model and sampling from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-listing",
   "metadata": {},
   "source": [
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "2) Reweighting the distribution to a certain “temperature”\n",
    "3) Sampling the next character at random according to the reweighted distribution\n",
    "4) Adding the new character at the end of the available text \n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model,\n",
    "and draw a character index from it (the “sampling function”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dress-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-france",
   "metadata": {},
   "source": [
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compatible-print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "1565/1565 [==============================] - 190s 120ms/step - loss: 2.2394\n",
      "--- Generating with seed: \"es to solve). on the part of\n",
      "pious, or merely church-going p\"\n",
      "------ temperature: 0.2\n",
      "es to solve). on the part of\n",
      "pious, or merely church-going promes of the disting the spirit in the present and the sense of the prome and think of the present and the distory and thinks of the promes of the self-concenting things and sense of the condection and in the man to be thinks in the self-and think of the individual and instinction of the experies in the been and think of the great of the present of the different in the present in the promes and th\n",
      "------ temperature: 0.5\n",
      "present of the different in the present in the promes and things of the must of the lears of the present are in the but think as in the specious in the believe in a such a bene in the many into exeration and all he one of the methous of must his interrication of this and pose to in the believe in the great at the spirit and deed may interplience to things a featting to be think of thinks of this one of self--a think and his such at every senvenow of the ex\n",
      "------ temperature: 1.0\n",
      "ne of self--a think and his such at every senvenow of the extepter to alsayd finded sty hadophou, in think bedicction which their, himolg intend\"ursing,\n",
      "in his condcient of yy advocie leous its hesours of can ourcesse belic- are has\n",
      "greatt who had\n",
      "oright\n",
      "ourable undiefinalgen of do thinkore it thr a concencioually nor the meacly. rowed we trode: sing\n",
      " being he jungerary. and of slank of the fuith he\n",
      "owh only the good, but a pance of for astolay consequent,\n",
      "------ temperature: 1.2\n",
      "he\n",
      "owh only the good, but a pance of for astolay consequent, to creat which hip\n",
      "sayppence; meneing latirining them: man,\n",
      "cormordwithousfm.\n",
      "\n",
      "b foind. he\n",
      "whomresthe\n",
      "to suf,\n",
      "the indecimitudredare for nem is bne! it a compack that pentive find and only mush, in b4d athire intile liy tilalia.ute justly til raite godsurex; : this an aw heurs to ob[jy! itsom(eoutinely a end atele! dovex , a\n",
      "yoon my himself which\n",
      "it, hnoral\n",
      "are mude pariow--orod id\n",
      "intirive a, do\n",
      "\n",
      "epoch 2\n",
      "1565/1565 [==============================] - 187s 120ms/step - loss: 1.6100\n",
      "--- Generating with seed: \"od, who is the sentinel and witness of every act, every mome\"\n",
      "------ temperature: 0.2\n",
      "od, who is the sentinel and witness of every act, every momerned to the suffering the spirit in the spirit to the sublight to the spirit is a perhaps an and the spirit in the soul it is the subelest the subeled to the decise the soul the spirit in the spirit to the spirit to the spirit of the strange of the spirit and the spirit to the spirit and the strange to the spirit to the spirit to the spirit to the subeled to the spirit in the spirit in the substio\n",
      "------ temperature: 0.5\n",
      "it to the subeled to the spirit in the spirit in the substions, it is the most prestans the\n",
      "greater and chorce of him a suffering to be therefore the berefore its an and with the honomenness to the same\n",
      "present the great should not supposity\n",
      "and submit of the lack of the distooler and their something the speak of the should any them is the belief for preduction of subligned and far to the science for the\n",
      "fact themselves to one modern there one is man to th\n",
      "------ temperature: 1.0\n",
      "for the\n",
      "fact themselves to one modern there one is man to the same\n",
      "itself gow.--tyman eight of the peris the areter of one hand.--austo instincts of the\n",
      "frind experied to dilace, would ofor that in\n",
      "the himmer, by belimation there\n",
      "indicts are for the  what is the periodue and juits in the necessity--growtimate spiritual anger if of a wared, hulk us an viegorful very of a man there and that does the\n",
      "exists theoself--must be nor he actirates a \"too them\n",
      "ormen\n",
      "------ temperature: 1.2\n",
      "\n",
      "exists theoself--must be nor he actirates a \"too them\n",
      "ormentd\n",
      "will nawlest theosemanant afting, sure such barvance, to degreekg; but\n",
      "ideag, an pranfingroses, windrehister duferestity, a new\n",
      "fanicrita.----hutorches mobeler. fulttrodanged, ane\n",
      "super-itman, like lince him: has sole every growtumeed\n",
      "percois, we, thereffect comed to as rewaring:e, kfo--there istonaesed\n",
      "trifey ios, no becoces? ther hentements perialed, hupenced? intragonation, lect implitions),\n",
      "epoch 3\n",
      "1565/1565 [==============================] - 194s 124ms/step - loss: 1.5231\n",
      "--- Generating with seed: \"uper-abundance, the protection are there lacking under which\"\n",
      "------ temperature: 0.2\n",
      "uper-abundance, the protection are there lacking under which is the strength and the the states and the same the present and the sublight of the sublight to the state and the spirit and intermition of the sense of the state of the same the same and the same that is a spirit and the state of the same the sense of the enthures of the state of the same the man and the the the state of the states the stand the subjegation of the state of the same as a sublight\n",
      "------ temperature: 0.5\n",
      "stand the subjegation of the state of the same as a sublight, the staller there is a mader, as a soul, the helication of the individual signification of the serpors, and all fautation and\n",
      "your destruition of the sublight, in the consequent and the emolity and some passion, which is the decourse and the the charm of the spirits of the master of the life of the present which is present which is the destructed have station that and the soul in the individual \n",
      "------ temperature: 1.0\n",
      "destructed have station that and the soul in the individual intersention of prible, but, that their wother of her-is which, compassion, hraog? though, who will the wat moralsting\n",
      "and waw never howretoing that we also the prewarity, learnee tzedelf for a possess is necessaprespressing that was\n",
      "human flithe\n",
      "how have he wimable main there may is all the requirestr of man\"--your instincts it from ran distancian cipal of the sensibility clamible, that he philos\n",
      "------ temperature: 1.2\n",
      "distancian cipal of the sensibility clamible, that he philosophazing are in the valuality- and egoiset, the m: vilove\n",
      "pry\n",
      "to the fuith this\n",
      "in ourtboolozd, who\n",
      " vali-mentifoue, that womaticigatic of human moral loys other cladundivess\" even the facition\n",
      "who\n",
      "thost: was the table betwould iotherderary.qioge vigwex alwabner\n",
      "re divid.t\"?\n",
      "lwhyrace: hild, brreamitverath, as that physiows rrbe:tur of\n",
      "command, sufferity for lixe towaowsted\n",
      "an cumfelus contemm\n",
      "that\n",
      "epoch 4\n",
      "1565/1565 [==============================] - 200s 128ms/step - loss: 1.4767\n",
      "--- Generating with seed: \"express\n",
      "accurately what all these masters of new modes of sp\"\n",
      "------ temperature: 0.2\n",
      "express\n",
      "accurately what all these masters of new modes of spirit and the subjuctive of the supersious the also and the supersious the more the more also the world the contemption of the contemption of the supersious the supersious for the supersious and some more the supersious more the supersious of the contemption of the supersious the contemption of the supersious in the supersious the faculture of the supersious and the supersious and the supersious an\n",
      "------ temperature: 0.5\n",
      "e of the supersious and the supersious and the supersious and supersation and feeling of a something and all the called. if the world and really the bensised the same instance, and because the sublime must be the new sthing and who has the conception of the same order the inclined one more curression of all the profounded and man long and all its conception of concess and is the starser the stronger and free spirit which be longer who has a religious and a\n",
      "------ temperature: 1.0\n",
      "er and free spirit which be longer who has a religious and among is the feelium, world, as the guelt gradions, headang, what explaed, the europead, in our science for the sectede end\n",
      "grastic word how them astempt to be uponly considerately, the trufferent, afcorationfound. longer clight of a\n",
      "falsouses: (as well from a sunes not would\n",
      "be all hrilly man be before the cormonance of -the\n",
      "whole op\n",
      "dingle in mentide,\" as they appcedvors its when animally to the \n",
      "------ temperature: 1.2\n",
      "le in mentide,\" as they appcedvors its when animally to the turneson, i de bill the valuability.\n",
      "suchon evinally if\n",
      "ye to swhole who strementher, though byleffeliness, this, his tritane open meful been imbely \"threutionfore of curtise).--here: as unmeto and shhupering and shysimen's dion,\n",
      "or woman without here berad the maj commened\n",
      "rotheled advidian magrify schopainly\n",
      "proupt\n",
      "supershomlun, that\n",
      "bencine pleasured to the uniuted in a confisifued.\"\n",
      "and othep\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "    \n",
    "# Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "# We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "                \n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-charity",
   "metadata": {},
   "source": [
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as “eterned” or “troveration”). With a high temperature, the local structure starts\n",
    "breaking down and most words look like semi-random strings of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-promise",
   "metadata": {},
   "source": [
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and realistic than ours. But of course, don’t expect to ever generate any meaningful text, other than by random chance: all we are doing is sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic statistical structure, thus making it impossible to\n",
    "learn a language model like we just did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-ceramic",
   "metadata": {},
   "source": [
    "#### Take aways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-cleaning",
   "metadata": {},
   "source": [
    "1) We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
    "2) In the case of text, such a model is called a “language model” and could be based on either words or characters.\n",
    "3) Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
    "4) One way to handle this is the notion of softmax temperature. Always experiment with different temperatures to find the “right” one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-damages",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
