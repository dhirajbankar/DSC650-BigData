{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handled-calvin",
   "metadata": {},
   "source": [
    "#### Text generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-sector",
   "metadata": {},
   "source": [
    "This notebook contains the code samples found in Chapter 8, Section 1 of Deep Learning with Python. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-sessions",
   "metadata": {},
   "source": [
    "#### Implementing character-level LSTM text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-memorial",
   "metadata": {},
   "source": [
    "Let’s put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files – Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche’s writing style and topics of choice, rather than a more generic model of the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-secretariat",
   "metadata": {},
   "source": [
    "Let’s start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-weight",
   "metadata": {},
   "source": [
    "##### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alert-manchester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-grade",
   "metadata": {},
   "source": [
    "Next, we will extract partially-overlapping sequences of length maxlen, one-hot encode them and pack them in a 3D Numpy array x of shape (sequences, maxlen, unique_characters). Simultaneously, we prepare a array y containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legal-humanity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-color",
   "metadata": {},
   "source": [
    "##### Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-insight",
   "metadata": {},
   "source": [
    "Our network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters. But let us note that recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "false-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-expert",
   "metadata": {},
   "source": [
    "Since our targets are one-hot encoded, we will use categorical_crossentropy as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accurate-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-immune",
   "metadata": {},
   "source": [
    "##### Training the language model and sampling from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-technique",
   "metadata": {},
   "source": [
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "2) Reweighting the distribution to a certain “temperature”\n",
    "3) Sampling the next character at random according to the reweighted distribution\n",
    "4) Adding the new character at the end of the available text \n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model,\n",
    "and draw a character index from it (the “sampling function”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sitting-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-owner",
   "metadata": {},
   "source": [
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heavy-herald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "1565/1565 [==============================] - 201s 127ms/step - loss: 2.2686\n",
      "--- Generating with seed: \"he motley whirl of the senses--the\n",
      "mob of the senses, as pla\"\n",
      "------ temperature: 0.2\n",
      "he motley whirl of the senses--the\n",
      "mob of the senses, as plated and the strenged and the stand and the strenged and the something the presention of the something the strenged the stranger of the something the stand of the some the still the something the strance of the conceation, the strangeration, and the stand of the sense and the strenged and the same the sense of the still the stand of the stand is is and the strenged and the stand and all the somethi\n",
      "------ temperature: 0.5\n",
      "and is is and the strenged and the stand and all the something and the follning the ighals in the discruesed to the greative except the same evinion of the relacting, and who learn, the something the have the sume and form, and regoods as is more of his of in the something, when a morality strong the\n",
      "something the stunding the morality of the will stands the power he as to the cate them the specied the also also not which the some of the\n",
      "stand whe has whic\n",
      "------ temperature: 1.0\n",
      "d the also also not which the some of the\n",
      "stand whe has which. precessine with eo-gide oftherspe, by the gakned by species he name has religes but pact, whether--is to us indestoness. leart hremany of which as nothin mosimenessnesty--if at mank of incallefalition, sensethed away excepal\n",
      "compartrally's; that nosore alove\n",
      "\"sperns in agains the gorthile, cturtinabit: what other ouring atnons, from alove himself.!\n",
      "\n",
      "        \" nomsa: that frutm, of learasory cla\n",
      "------ temperature: 1.2\n",
      "ove himself.!\n",
      "\n",
      "        \" nomsa: that frutm, of learasory claps\" ids of then itstled, chuls.=--e:very more deperves, pyemofuress flamedustco of theyekne, which oncem. gowl: and smculity dpenter\n",
      "foolewhas to us ans ned noid, in issaliys's for doy rations worm nlthils dried still highous whith views which\n",
      "surdnd; and\n",
      "sompthing-to thothhed, as at knowly ordelen dening hauscrom, the self-prese knots, he presenties.\"-it and amend, cbresueveb)r, and germantlesily\n",
      "epoch 2\n",
      "1565/1565 [==============================] - 197s 126ms/step - loss: 1.6173\n",
      "--- Generating with seed: \"-charged with interrogative signs--and\n",
      "often sick unto death\"\n",
      "------ temperature: 0.2\n",
      "-charged with interrogative signs--and\n",
      "often sick unto deather of the all the consend to the would be a present the reason to the sense of the believed and all the believed and the common as the suble to the been struggless of sufferent to the vistous and say as a power of the would all the existence and the subter and the sense and say the would the suble to the common to the world as a present the been the wart as the suble and the same and the have the \n",
      "------ temperature: 0.5\n",
      "he been the wart as the suble and the same and the have the are now more ellows and the states a compare of god intermoral makes and with every the best of the same about the action of the us to the spirit and more viese the hourselves of a distime in the more one self-astic and to spectiain the world respect of the very retood and as a power--the origin, and commous age and vision of exceplation of every religious intention of all the present himself to b\n",
      "------ temperature: 1.0\n",
      "of every religious intention of all the present himself to be kinds, in itself who maist courdly. the soul us--which condiced\n",
      "men in\n",
      "also asciency as megnifulse soothers which ary in\n",
      "ruled to the coursent should feety and the mothest tenn in\n",
      "itself into fimure, them more as commance more stragge of respend of the betiess entermital to himself to what is procauarisions.--no ward\n",
      "present reself, is itself all comations and not skops, he the emver-yotheity co\n",
      "------ temperature: 1.2\n",
      "itself all comations and not skops, he the emver-yotheity condrations with it loos,\n",
      "should withing wath tregurely. affearome indyraed tyigh equal\n",
      "our\n",
      "wlouconspic votury, lack falh our with necess the gevory\n",
      "futual juchs: as noer his psicors each on enlocation but one  could ourselves. the mots sinse cariy\n",
      "chupur  moleble (it so frothing\n",
      "much as to great kisth formorable\", as soomy over of onjoraus with ics amy ondied and,-indiviate-miveds vory one goich ar\n",
      "epoch 3\n",
      "1565/1565 [==============================] - 197s 126ms/step - loss: 1.5284\n",
      "--- Generating with seed: \"annihilation, petrifaction, a hatred surmounting love, perha\"\n",
      "------ temperature: 0.2\n",
      "annihilation, petrifaction, a hatred surmounting love, perhaps and the self-conscious and the most the conscious and the stranged and the most the most and the most selfly the result of the most the rest as a preciation of the superie and the stranged to the most only the self-conduced be the stranged to the most and the self and the demistion of the futures of the self proved the rest as it were the more the more of the most once is the souls and the most\n",
      "------ temperature: 0.5\n",
      "the more the more of the most once is the souls and the most also to a germans it who distive\n",
      "at mosing to its the duch as it would not itself and germans to be promite itself--not without the standard and and the restruced the breaked and its of the higher, the souls and the action is the possestically and heart of the selfless to be so fact of\n",
      "the doubt of the day the reals. he would not enthuling and the something that it is the a destrustic of the supe\n",
      "------ temperature: 1.0\n",
      "ng and the something that it is the a destrustic of the supersotful\n",
      "and intections in the world tib1, of whom even the be has\n",
      "and amcondity of the wettent, an any.\n",
      "\n",
      "                           in to bit out is exceptingly and in any respolses of the souls\n",
      "asso reeusic\n",
      "talk, bedy inmay emos\n",
      "of\n",
      "the internomution of dipidences entirefulness through the cause of the maint under to atsucs andisted whreavene his workstis of \"welling errlely are assemuate whoke th\n",
      "------ temperature: 1.2\n",
      "vene his workstis of \"welling errlely are assemuate whoke they b:iedly but entifury\n",
      "with a tempole\"--podsis. and\n",
      "\"truth, it comebjues of thot sprinens.\n",
      "i mode-inived to venu\n",
      "except with it, or alsampe.\n",
      "tasted. it maskind, wholvanced w\"rigining as bette. thinever eit he world you\n",
      "of origin the\n",
      "senses too which every ununempy if alnod to she thing dearly way of\n",
      "diverseful, that cercoriliaris we ted to have distumbty and should a test\n",
      "finsidedr, -it migutener\n",
      "epoch 4\n",
      "1565/1565 [==============================] - 197s 126ms/step - loss: 1.4811\n",
      "--- Generating with seed: \" gradually got so far,\n",
      "that one no longer lets the consequen\"\n",
      "------ temperature: 0.2\n",
      " gradually got so far,\n",
      "that one no longer lets the consequences of morality of the propided that the conscience of its own distinct, and a soul, and a conscience and the present and soul and a more that the souls and the spiritual and the find and the present the concerning and the souls of the sense of the self a more pared and the spiritual to a more man is the sense of the pride of the spiritual and the soul and the sense and the be the person and a sou\n",
      "------ temperature: 0.5\n",
      "l and the soul and the sense and the be the person and a soul, and the spiritual\n",
      "gain believe that is religion of its who has been that one man as its own soul as the most preciselve to please of the seems\n",
      "he is this only the most conssious and intereation of the morality and a thing of we thouth the unimmose the seriat as an art of the senders of the make a platour consciention of his was to so deserve of the acts to be acts some man, the persons of the f\n",
      "------ temperature: 1.0\n",
      "eserve of the acts to be acts some man, the persons of the foodeftime fritter, artistms to taste to the seads \" that deex--long. this other futs that himself\n",
      "borier of with -his solong,\n",
      "the\n",
      "shord\n",
      "such ceet elsed bring evolve, inishers of those that lighted femitie, the higher regard also expecien, a respicis,\n",
      "in the sublencess is\n",
      "im always attempte to bes\n",
      "a\n",
      "desires of the howel as self-impresspanad are self-converses thare in at harden is as\n",
      "science of cor\n",
      "------ temperature: 1.2\n",
      "d are self-converses thare in at harden is as\n",
      "science of corcaer: allowing shurient turded \"faidry thathis origine, far xitable,\" power' is indicaltous,\n",
      "nearly turdn andile!\n",
      "it is an hroilizir of hoge sympaths.\"\", superially;\n",
      "and\n",
      "today\" of then denieation, efver abrest, to-day plys of ly\": poseless-nr\"\n",
      "that have exease detain thar point musth eathing, that art then that is parature: it is a tender inmores it, so of historical to tastic sparagy yous, will r\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "    \n",
    "# Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "# We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "                \n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-humidity",
   "metadata": {},
   "source": [
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as “eterned” or “troveration”). With a high temperature, the local structure starts\n",
    "breaking down and most words look like semi-random strings of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-timothy",
   "metadata": {},
   "source": [
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and realistic than ours. But of course, don’t expect to ever generate any meaningful text, other than by random chance: all we are doing is sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic statistical structure, thus making it impossible to\n",
    "learn a language model like we just did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-intermediate",
   "metadata": {},
   "source": [
    "#### Take aways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-walnut",
   "metadata": {},
   "source": [
    "1) We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
    "2) In the case of text, such a model is called a “language model” and could be based on either words or characters.\n",
    "3) Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
    "4) One way to handle this is the notion of softmax temperature. Always experiment with different temperatures to find the “right” one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-frontier",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
